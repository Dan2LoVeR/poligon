# Разделяем текст на документы (предполагаем, что каждый документ - это абзац)
documents = [paragraph for paragraph in data.split('\n') if paragraph.strip()]
# Разбиваем исходный текст по символам новой строки (\n)
# paragraph.strip() проверяет, что строка не пустая (после удаления пробелов)
# В результате получаем список абзацев как отдельных документов

# Функция для предобработки документа
def preprocess_document(doc):
    words = re.findall(r'\b[а-яё]+\b', doc.lower())
    # Аналогично очищаем документ: оставляем только русские слова в нижнем регистре
    return lemmatize(words)
    # Возвращаем лемматизированные слова документа

# Создаем список всех уникальных слов в корпусе
all_words = set()
# Используем множество для автоматического удаления дубликатов

for doc in documents:
    all_words.update(preprocess_document(doc))
    # Для каждого документа добавляем его уникальные слова в общее множество

all_words = list(all_words)
# Преобразуем множество обратно в список

# Создаем векторные представления документов
from collections import Counter
# Импортируем Counter для подсчета частот слов

vector_model = []
# Создаем пустой список для хранения векторных представлений

for doc in documents:
    words = preprocess_document(doc)
    # Получаем очищенные и лемматизированные слова документа
    word_counts = Counter(words)
    # Считаем частоту каждого слова в документе
    vector = [word_counts.get(word, 0) for word in all_words]
    # Создаем вектор: для каждого слова из общего списка указываем его частоту в документе
    # Если слова нет в документе, ставим 0
    vector_model.append(vector)
    # Добавляем вектор документа в общую модель

print(f"Количество документов: {len(vector_model)}")
print(f"Размерность векторов: {len(vector_model[0])}")
# Выводим информацию о количестве документов и размерности векторов