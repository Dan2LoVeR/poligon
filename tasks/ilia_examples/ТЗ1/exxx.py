
# Импорт модуля re для работы с регулярными выражениями
# Импорт defaultdict из модуля collections для создания словарей со значениями по умолчанию

import re

# Загрузка данных из файла
with open('tz1_data.txt', 'r', encoding='utf-8') as f:
    # Чтение всех строк из файла, удаление пробелов в начале и конце каждой строки
    # и сохранение только непустых строк в список sentences
    sentences = [line.strip() for line in f if line.strip()]


# Функция для простой предобработки текста
def simple_preprocess(text):
    # Удаление всех символов, кроме букв, цифр и пробелов (знаки препинания удаляются)
    # и приведение текста к нижнему регистру
    text = re.sub(r'[^\w\s]', '', text.lower())
    return text


# Применение функции предобработки к каждому предложению в списке
processed_sentences = [simple_preprocess(sent) for sent in sentences]


# Функция для классификации предложений по значению слова "кисть"
def classify_sentence(sentence):
    # Списки ключевых слов для каждого из трех значений слова "кисть":

    # Слова, связанные с частью тела
    body_parts = ['рука', 'ладонь', 'запястье', 'локоть']

    # Слова, связанные с инструментами (кисть для рисования, макияжа и т.д.)
    tools = ['рисован', 'макияж', 'пудр', 'крас', 'брить', 'художник', 'мазк']

    # Слова, связанные с растениями (соцветиями)
    plants = ['сирен', 'калин', 'черешн', 'смородин', 'крыжовник', 'рябин', 'цвет', 'ягод']

    # Разбиваем предложение на отдельные слова
    words = sentence.split()

    # Подсчет количества слов, связанных с частью тела
    body_count = sum(1 for word in words if word in body_parts)

    # Подсчет количества слов, связанных с инструментами (проверяем вхождение подстрок)
    tool_count = sum(1 for word in words if any(tool in word for tool in tools))

    # Подсчет количества слов, связанных с растениями (проверяем вхождение подстрок)
    plant_count = sum(1 for word in words if any(plant in word for plant in plants))

    # Создаем словарь с количеством совпадений для каждого класса
    counts = {
        0: body_count,  # часть тела
        1: tool_count,  # инструмент
        2: plant_count  # соцветие
    }

    # Возвращаем класс с максимальным количеством совпадений
    return max(counts.items(), key=lambda x: x[1])[0]


# Классификация всех предложений с помощью функции classify_sentence
auto_classes = [classify_sentence(sent) for sent in processed_sentences]

# Ручная классификация предложений (предполагается, что она уже была сделана)
manual_classes = [
    2, 2, 0, 1, 0, 1, 0, 0, 0, 1,
    2, 1, 0, 2, 1, 1, 1, 0, 2, 0,
    0, 0, 2
]

# Сравнение автоматической классификации с ручной
correct = sum(1 for a, m in zip(auto_classes, manual_classes) if a == m)
# Вычисление точности как доли правильных ответов
accuracy = correct / len(manual_classes)

# Вывод результатов классификации
print("Результаты классификации:")
# Словарь для преобразования числовых меток классов в читаемые названия
class_names = {0: "Часть тела", 1: "Инструмент", 2: "Соцветие"}
# Вывод каждого предложения с его классификацией
for i, (sent, cls) in enumerate(zip(sentences, auto_classes)):
    print(f"{class_names[cls]}: {sent}")

# Вывод точности классификации в процентах с одним знаком после запятой
print(f"\nТочность классификации: {accuracy:.1%}")

# Создание кластеров на основе автоматической классификации
clusters = auto_classes

# Вывод кластеров (групп предложений)
print("\nКластеры (группы):")
# Для каждого уникального идентификатора кластера
for cluster_id in set(clusters):
    # Вывод заголовка кластера с его названием
    print(f"\nКластер {cluster_id} ({class_names[cluster_id]}):")
    # Вывод всех предложений, относящихся к данному кластеру
    for i, c in enumerate(clusters):
        if c == cluster_id:
            print(f"  - {sentences[i]}")